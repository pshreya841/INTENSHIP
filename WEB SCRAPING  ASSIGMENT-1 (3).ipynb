{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f89e723",
   "metadata": {},
   "source": [
    "#                                  ASSIGMENT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda7057",
   "metadata": {},
   "source": [
    "WEB SCRAPING ASSIGMENT -1"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fee4ec8b",
   "metadata": {},
   "source": [
    "QUE:-1\n",
    "1) Write a python program to display all the header tags from wikipedia.org and make data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75d590d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List all the header tags :\n",
      "\n",
      "<h1 class=\"firstHeading mw-first-heading\" id=\"firstHeading\" style=\"display: none\"><span class=\"mw-page-title-main\">Main Page</span></h1>\n",
      "\n",
      "<h1><span class=\"mw-headline\" id=\"Welcome_to_Wikipedia\">Welcome to <a href=\"/wiki/Wikipedia\" title=\"Wikipedia\">Wikipedia</a></span></h1>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfa-h2\"><span id=\"From_today.27s_featured_article\"></span><span class=\"mw-headline\" id=\"From_today's_featured_article\">From today's featured article</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-dyk-h2\"><span class=\"mw-headline\" id=\"Did_you_know_...\">Did you know ...</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-itn-h2\"><span class=\"mw-headline\" id=\"In_the_news\">In the news</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-otd-h2\"><span class=\"mw-headline\" id=\"On_this_day\">On this day</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-tfp-h2\"><span id=\"Today.27s_featured_picture\"></span><span class=\"mw-headline\" id=\"Today's_featured_picture\">Today's featured picture</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-other\"><span class=\"mw-headline\" id=\"Other_areas_of_Wikipedia\">Other areas of Wikipedia</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-sister\"><span id=\"Wikipedia.27s_sister_projects\"></span><span class=\"mw-headline\" id=\"Wikipedia's_sister_projects\">Wikipedia's sister projects</span></h2>\n",
      "\n",
      "<h2 class=\"mp-h2\" id=\"mp-lang\"><span class=\"mw-headline\" id=\"Wikipedia_languages\">Wikipedia languages</span></h2>\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "html = urlopen('https://en.wikipedia.org/wiki/Main_Page')\n",
    "bs = BeautifulSoup(html, \"html.parser\")\n",
    "titles = bs.find_all(['h1', 'h2','h3','h4','h5','h6'])\n",
    "print('List all the header tags :', *titles, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da2f4049",
   "metadata": {},
   "source": [
    "QUE:2\n",
    "2)Write s python program to display list of respected former presidents of India(i.e. Name , Term ofoffice)\n",
    "from https://presidentofindia.nic.in/former-presidents.htm and make data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1cdb77bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table not found on the webpage.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = 'https://presidentofindia.nic.in/former-presidents.htm'\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create BeautifulSoup object to parse the HTML\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table containing the list of former presidents\n",
    "table = soup.find('table', {'class': 'tablepress'})\n",
    "\n",
    "# Check if the table is found\n",
    "if table is None:\n",
    "    print(\"Table not found on the webpage.\")\n",
    "else:\n",
    "    # Create empty lists to store the data\n",
    "    names = []\n",
    "    terms = []\n",
    "\n",
    "    # Extract the data from the table rows\n",
    "    for row in table.find_all('tr')[1:]:\n",
    "        columns = row.find_all('td')\n",
    "        name = columns[0].text.strip()\n",
    "        term = columns[1].text.strip()\n",
    "        names.append(name)\n",
    "        terms.append(term)\n",
    "\n",
    "    # Create a DataFrame from the extracted data\n",
    "    data = {'Name': names, 'Term of Office': terms}\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "85928b0a",
   "metadata": {},
   "source": [
    "QUE:-3\n",
    "3) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\u0002\n",
    "a) Top 10 ODI teams in men’s cricket along with the records for matches, points and rating.\n",
    "b) Top 10 ODI Batsmen along with the records of their team andrating.\n",
    "c) Top 10 ODI bowlers along with the records of their team andrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e99b35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams:\n",
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      23  2,714    118\n",
      "1     Pakistan\\nPAK      20  2,316    116\n",
      "2        India\\nIND      33  3,807    115\n",
      "3   New Zealand\\nNZ      27  2,806    104\n",
      "4      England\\nENG      24  2,426    101\n",
      "5  South Africa\\nSA      19  1,910    101\n",
      "6   Bangladesh\\nBAN      25  2,451     98\n",
      "7  Afghanistan\\nAFG      10    878     88\n",
      "8     Sri Lanka\\nSL      21  1,682     80\n",
      "9   West Indies\\nWI      25  1,797     72\n",
      "\n",
      "Top 10 ODI Batsmen:\n",
      "                 Batsman Team Rating\n",
      "0             Babar Azam  PAK    886\n",
      "1  Rassie van der Dussen   SA    777\n",
      "2           Fakhar Zaman  PAK    755\n",
      "3            Imam-ul-Haq  PAK    745\n",
      "4           Shubman Gill  IND    738\n",
      "5           David Warner  AUS    726\n",
      "6           Harry Tector  IRE    722\n",
      "7            Virat Kohli  IND    719\n",
      "8        Quinton de Kock   SA    718\n",
      "9           Rohit Sharma  IND    707\n",
      "\n",
      "Top 10 ODI Bowlers:\n",
      "             Bowler Team Rating\n",
      "0    Josh Hazlewood  AUS    705\n",
      "1    Mohammed Siraj  IND    691\n",
      "2    Mitchell Starc  AUS    686\n",
      "3        Matt Henry   NZ    667\n",
      "4       Trent Boult   NZ    660\n",
      "5       Rashid Khan  AFG    659\n",
      "6        Adam Zampa  AUS    652\n",
      "7  Mujeeb Ur Rahman  AFG    637\n",
      "8     Mohammad Nabi  AFG    631\n",
      "9    Shaheen Afridi  PAK    630\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape and create DataFrame for ODI teams\n",
    "def scrape_odi_teams():\n",
    "    url = 'https://www.icc-cricket.com/rankings/mens/team-rankings/odi'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "    \n",
    "    teams = []\n",
    "    matches = []\n",
    "    points = []\n",
    "    ratings = []\n",
    "    \n",
    "    for row in table.find_all('tr')[1:11]:\n",
    "        columns = row.find_all('td')\n",
    "        team = columns[1].text.strip()\n",
    "        match = columns[2].text.strip()\n",
    "        point = columns[3].text.strip()\n",
    "        rating = columns[4].text.strip()\n",
    "        \n",
    "        teams.append(team)\n",
    "        matches.append(match)\n",
    "        points.append(point)\n",
    "        ratings.append(rating)\n",
    "    \n",
    "    data = {'Team': teams, 'Matches': matches, 'Points': points, 'Rating': ratings}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Function to scrape and create DataFrame for top 10 ODI batsmen\n",
    "def scrape_odi_batsmen():\n",
    "    url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/batting'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "    \n",
    "    batsmen = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "    \n",
    "    for row in table.find_all('tr')[1:11]:\n",
    "        columns = row.find_all('td')\n",
    "        batsman = columns[1].text.strip()\n",
    "        team = columns[2].text.strip()\n",
    "        rating = columns[3].text.strip()\n",
    "        \n",
    "        batsmen.append(batsman)\n",
    "        teams.append(team)\n",
    "        ratings.append(rating)\n",
    "    \n",
    "    data = {'Batsman': batsmen, 'Team': teams, 'Rating': ratings}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Function to scrape and create DataFrame for top 10 ODI bowlers\n",
    "def scrape_odi_bowlers():\n",
    "    url = 'https://www.icc-cricket.com/rankings/mens/player-rankings/odi/bowling'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "    \n",
    "    bowlers = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "    \n",
    "    for row in table.find_all('tr')[1:11]:\n",
    "        columns = row.find_all('td')\n",
    "        bowler = columns[1].text.strip()\n",
    "        team = columns[2].text.strip()\n",
    "        rating = columns[3].text.strip()\n",
    "        \n",
    "        bowlers.append(bowler)\n",
    "        teams.append(team)\n",
    "        ratings.append(rating)\n",
    "    \n",
    "    data = {'Bowler': bowlers, 'Team': teams, 'Rating': ratings}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Scrape and print DataFrame for top 10 ODI teams\n",
    "odi_teams_df = scrape_odi_teams()\n",
    "print(\"Top 10 ODI Teams:\")\n",
    "print(odi_teams_df)\n",
    "\n",
    "# Scrape and print DataFrame for top 10 ODI batsmen\n",
    "odi_batsmen_df = scrape_odi_batsmen()\n",
    "print(\"\\nTop 10 ODI Batsmen:\")\n",
    "print(odi_batsmen_df)\n",
    "\n",
    "# Scrape and print DataFrame for top 10 ODI bowlers\n",
    "odi_bowlers_df = scrape_odi_bowlers()\n",
    "print(\"\\nTop 10 ODI Bowlers:\")\n",
    "print(odi_bowlers_df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cf277c9a",
   "metadata": {},
   "source": [
    "QUE:-4\n",
    "4) Write a python program to scrape cricket rankings from icc-cricket.com. You have to scrape and make data frame\u0002\n",
    "a) Top 10 ODI teams in women’s cricket along with the records for matches, points and rating.\n",
    "b) Top 10 women’s ODI Batting players along with the records of their team and rating.\n",
    "c) Top 10 women’s ODI all-rounder along with the records of their team and rating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d4ffa32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 ODI Teams in Women's Cricket:\n",
      "               Team Matches Points Rating\n",
      "0    Australia\\nAUS      21  3,603    172\n",
      "1      England\\nENG      28  3,342    119\n",
      "2  South Africa\\nSA      26  3,098    119\n",
      "3        India\\nIND      27  2,820    104\n",
      "4   New Zealand\\nNZ      25  2,553    102\n",
      "5   West Indies\\nWI      27  2,535     94\n",
      "6     Thailand\\nTHA      11    821     75\n",
      "7   Bangladesh\\nBAN      14    977     70\n",
      "8     Pakistan\\nPAK      27  1,678     62\n",
      "9     Sri Lanka\\nSL       9    479     53\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Function to scrape and create DataFrame for top 10 ODI teams in women's cricket\n",
    "def scrape_odi_women_teams():\n",
    "    url = 'https://www.icc-cricket.com/rankings/womens/team-rankings/odi'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "    \n",
    "    teams = []\n",
    "    matches = []\n",
    "    points = []\n",
    "    ratings = []\n",
    "    \n",
    "    for row in table.find_all('tr')[1:11]:\n",
    "        columns = row.find_all('td')\n",
    "        team = columns[1].text.strip()\n",
    "        match = columns[2].text.strip()\n",
    "        point = columns[3].text.strip()\n",
    "        rating = columns[4].text.strip()\n",
    "        \n",
    "        teams.append(team)\n",
    "        matches.append(match)\n",
    "        points.append(point)\n",
    "        ratings.append(rating)\n",
    "    \n",
    "    data = {'Team': teams, 'Matches': matches, 'Points': points, 'Rating': ratings}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Function to scrape and create DataFrame for top 10 women's ODI batting players\n",
    "def scrape_women_odi_batting_players():\n",
    "    url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/batting'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "    \n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "    \n",
    "    for row in table.find_all('tr')[1:11]:\n",
    "        columns = row.find_all('td')\n",
    "        player = columns[1].text.strip()\n",
    "        team = columns[2].text.strip()\n",
    "        rating = columns[3].text.strip()\n",
    "        \n",
    "        players.append(player)\n",
    "        teams.append(team)\n",
    "        ratings.append(rating)\n",
    "    \n",
    "    data = {'Player': players, 'Team': teams, 'Rating': ratings}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Function to scrape and create DataFrame for top 10 women's ODI all-rounders\n",
    "def scrape_women_odi_allrounders():\n",
    "    url = 'https://www.icc-cricket.com/rankings/womens/player-rankings/odi/all-rounder'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    table = soup.find('table', {'class': 'table'})\n",
    "    \n",
    "    players = []\n",
    "    teams = []\n",
    "    ratings = []\n",
    "    \n",
    "    for row in table.find_all('tr')[1:11]:\n",
    "        columns = row.find_all('td')\n",
    "        player = columns[1].text.strip()\n",
    "        team = columns[2].text.strip()\n",
    "        rating = columns[3].text.strip()\n",
    "        \n",
    "        players.append(player)\n",
    "        teams.append(team)\n",
    "        ratings.append(rating)\n",
    "    \n",
    "    data = {'Player': players, 'Team': teams, 'Rating': ratings}\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Scrape and print DataFrame for top 10 ODI teams in women's cricket\n",
    "odi_women_teams_df = scrape_odi_women_teams()\n",
    "print(\"Top 10 ODI Teams in Women's Cricket:\")\n",
    "print(odi_women_teams_df)\n",
    "\n",
    "# Scrape and print DataFrame for top\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ef7b1d6",
   "metadata": {},
   "source": [
    "QUE:-5\n",
    "5) Write a python program to scrape mentioned news details from https://www.cnbc.com/world/?region=world and\n",
    "make data frame\u0002\n",
    "i) Headline\n",
    "ii) Time\n",
    "iii) News Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f89c464c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insurgents defeated after rare border raid - Russia\n",
      "Insurgents defeated after rare border raid - Russia\n",
      "Last known Taiwanese 'comfort woman' dies at 92\n",
      "Officer who Tasered 95-year-old woman suspended\n",
      "WhatsApp to allow users to edit messages\n",
      "Indian wrestlers risk Olympic dream for '#MeToo' protest\n",
      "Indian banks start exchanging withdrawn top currency\n",
      "Prince Harry loses challenge to pay for security\n",
      "Political row over India's new parliament opening\n",
      "India makes cough syrup testing mandatory for exports\n",
      "Comic Uncle Roger banned on Chinese social media\n",
      "TikTok sues to overturn first US state ban\n",
      "Comic Uncle Roger banned on Chinese social media\n",
      "TikTok sues to overturn first US state ban\n",
      "Juve docked 10 points over transfer dealings\n",
      "First Arab female astronaut reaches space station\n",
      "Star Wars and Thor actor Ray Stevenson dies at 58\n",
      "Watch: Mexican volcano spews ash into starry sky\n",
      "BBC World News TV\n",
      "BBC World Service Radio\n",
      "Ray Stevenson's film and TV career in pictures\n",
      "Escaped water buffalo herd wreck new swimming pool\n",
      "Why car parks are the hottest space in solar power\n",
      "Picnics and blooms: The Chelsea Flower Show in pictures\n",
      "Satellites reveal Russian defences before major assault\n",
      "Biden's no-show takes shine off Pacific pact\n",
      "Who are the fighters infiltrating Russia from Ukraine?\n",
      "Why DeSantis’s risky feud with Disney may pay off\n",
      "Intercepting Russian 'Zombies' at a Nato airbase\n",
      "UK rejects calls to return Ethiopian prince's body\n",
      "The woman who travelled seven hours by camel to give birth\n",
      "When saying condom aloud became birth control ad in India\n",
      "The 'flawed' skill that shaped history\n",
      "The big positives about AI at work\n",
      "The Dutch solution to stress\n",
      "The 100 greatest children's books ever\n",
      "The viruses hiding in our DNA\n",
      "Is talent or hard work more important?\n",
      "An iconic New York Jewish food\n",
      "News daily newsletter\n",
      "Mobile app\n",
      "Get in touch\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url='https://www.bbc.com/news'\n",
    "response = requests.get(url)\n",
    "\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "headlines = soup.find('body').find_all('h3')\n",
    "for x in headlines:\n",
    "    print(x.text.strip())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f78b9e38",
   "metadata": {},
   "source": [
    "QUE:-6\n",
    "6) Write a python program to scrape the details of most downloaded articles from AI in last 90\n",
    "days.https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\n",
    "Scrape below mentioned details and make data frame\u0002\n",
    "i) Paper Title\n",
    "ii) Authors\n",
    "iii) Published Date\n",
    "iv) Paper URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33f3e77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Paper Title  \\\n",
      "0                                    Reward is enough   \n",
      "1   Explanation in artificial intelligence: Insigh...   \n",
      "2              Creativity and artificial intelligence   \n",
      "3   Conflict-based search for optimal multi-agent ...   \n",
      "4   Knowledge graphs as tools for explainable mach...   \n",
      "5   Law and logic: A review from an argumentation ...   \n",
      "6   Between MDPs and semi-MDPs: A framework for te...   \n",
      "7   Explaining individual predictions when feature...   \n",
      "8       Multiple object tracking: A literature review   \n",
      "9   A survey of inverse reinforcement learning: Ch...   \n",
      "10  Evaluating XAI: A comparison of rule-based and...   \n",
      "11  Explainable AI tools for legal reasoning about...   \n",
      "12            Hard choices in artificial intelligence   \n",
      "13  Assessing the communication gap between AI mod...   \n",
      "14  Explaining black-box classifiers using post-ho...   \n",
      "15  The Hanabi challenge: A new frontier for AI re...   \n",
      "16              Wrappers for feature subset selection   \n",
      "17  Artificial cognition for social human–robot in...   \n",
      "18  A review of possible effects of cognitive bias...   \n",
      "19  The multifaceted impact of Ada Lovelace in the...   \n",
      "20  Robot ethics: Mapping the issues for a mechani...   \n",
      "21          Reward (Mis)design for autonomous driving   \n",
      "22  Planning and acting in partially observable st...   \n",
      "23  What do we want from Explainable Artificial In...   \n",
      "\n",
      "                                              Authors  Published Date  \\\n",
      "0   David Silver, Satinder Singh, Doina Precup, Ri...    October 2021   \n",
      "1                                          Tim Miller   February 2019   \n",
      "2                                   Margaret A. Boden     August 1998   \n",
      "3   Guni Sharon, Roni Stern, Ariel Felner, Nathan ...   February 2015   \n",
      "4                      Ilaria Tiddi, Stefan Schlobach    January 2022   \n",
      "5                      Henry Prakken, Giovanni Sartor    October 2015   \n",
      "6     Richard S. Sutton, Doina Precup, Satinder Singh     August 1999   \n",
      "7           Kjersti Aas, Martin Jullum, Anders Løland  September 2021   \n",
      "8                Wenhan Luo, Junliang Xing and 4 more      April 2021   \n",
      "9                       Saurabh Arora, Prashant Doshi     August 2021   \n",
      "10  Jasper van der Waa, Elisabeth Nieuwburg, Anita...   February 2021   \n",
      "11  Joe Collenette, Katie Atkinson, Trevor Bench-C...      April 2023   \n",
      "12   Roel Dobbe, Thomas Krendl Gilbert, Yonatan Mintz   November 2021   \n",
      "13  Oskar Wysocki, Jessica Katharine Davies and 5 ...      March 2023   \n",
      "14  Eoin M. Kenny, Courtney Ford, Molly Quinn, Mar...        May 2021   \n",
      "15          Nolan Bard, Jakob N. Foerster and 13 more      March 2020   \n",
      "16                         Ron Kohavi, George H. John   December 1997   \n",
      "17      Séverin Lemaignan, Mathieu Warnier and 3 more       June 2017   \n",
      "18    Tomáš Kliegr, Štěpán Bahník, Johannes Fürnkranz       June 2021   \n",
      "19                             Luigia Carlucci Aiello       June 2016   \n",
      "20             Patrick Lin, Keith Abney, George Bekey      April 2011   \n",
      "21     W. Bradley Knox, Alessandro Allievi and 3 more      March 2023   \n",
      "22  Leslie Pack Kaelbling, Michael L. Littman, Ant...        May 1998   \n",
      "23             Markus Langer, Daniel Oster and 6 more       July 2021   \n",
      "\n",
      "                                            Paper URL  \n",
      "0   https://www.sciencedirect.com/science/article/...  \n",
      "1   https://www.sciencedirect.com/science/article/...  \n",
      "2   https://www.sciencedirect.com/science/article/...  \n",
      "3   https://www.sciencedirect.com/science/article/...  \n",
      "4   https://www.sciencedirect.com/science/article/...  \n",
      "5   https://www.sciencedirect.com/science/article/...  \n",
      "6   https://www.sciencedirect.com/science/article/...  \n",
      "7   https://www.sciencedirect.com/science/article/...  \n",
      "8   https://www.sciencedirect.com/science/article/...  \n",
      "9   https://www.sciencedirect.com/science/article/...  \n",
      "10  https://www.sciencedirect.com/science/article/...  \n",
      "11  https://www.sciencedirect.com/science/article/...  \n",
      "12  https://www.sciencedirect.com/science/article/...  \n",
      "13  https://www.sciencedirect.com/science/article/...  \n",
      "14  https://www.sciencedirect.com/science/article/...  \n",
      "15  https://www.sciencedirect.com/science/article/...  \n",
      "16  https://www.sciencedirect.com/science/article/...  \n",
      "17  https://www.sciencedirect.com/science/article/...  \n",
      "18  https://www.sciencedirect.com/science/article/...  \n",
      "19  https://www.sciencedirect.com/science/article/...  \n",
      "20  https://www.sciencedirect.com/science/article/...  \n",
      "21  https://www.sciencedirect.com/science/article/...  \n",
      "22  https://www.sciencedirect.com/science/article/...  \n",
      "23  https://www.sciencedirect.com/science/article/...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = \"https://www.journals.elsevier.com/artificial-intelligence/most-downloaded-articles\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Initialize lists to store the details\n",
    "titles = []\n",
    "authors = []\n",
    "published_dates = []\n",
    "paper_urls = []\n",
    "\n",
    "# Extract the details of each article\n",
    "articles = soup.find(\"ul\", class_=\"sc-9zxyh7-0 cMKaMj\")\n",
    "\n",
    "for article in articles:\n",
    "    # Extract paper title\n",
    "    title = article.find(\"h2\").text.strip()\n",
    "    titles.append(title)\n",
    "    \n",
    "    # Extract authors' names\n",
    "    authors.append(article.find(\"span\", class_=\"sc-1w3fpd7-0 dnCnAO\").text.strip())\n",
    "    \n",
    "    # Extract published date\n",
    "    date = article.find(\"span\", class_=\"sc-1thf9ly-2 dvggWt\").text.strip()\n",
    "    published_dates.append(date)\n",
    "    \n",
    "    # Extract paper URL\n",
    "    url = article.find(\"a\", class_=\"sc-5smygv-0 fIXTHm\")[\"href\"]\n",
    "    paper_urls.append(url)\n",
    "\n",
    "# Create a dataframe with the extracted details\n",
    "data = {\n",
    "    \"Paper Title\": titles,\n",
    "    \"Authors\": authors,\n",
    "    \"Published Date\": published_dates,\n",
    "    \"Paper URL\": paper_urls\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "933ee7be",
   "metadata": {},
   "source": [
    "QUE:-7\n",
    "7) Write a python program to scrape mentioned details from dineout.co.inand make data frame\u0002i) Restaurant name\n",
    "ii) Cuisine\n",
    "iii) Location\n",
    "iv) Ratings\n",
    "v) Image UR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2546f6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Restaurant Name  \\\n",
      "0                             Local   \n",
      "1                           Tamasha   \n",
      "2                     My Bar Square   \n",
      "3                  Ministry Of Beer   \n",
      "4                    Openhouse Cafe   \n",
      "5                    Warehouse Cafe   \n",
      "6                Lord of the Drinks   \n",
      "7               Unplugged Courtyard   \n",
      "8                 The Junkyard Cafe   \n",
      "9                       Station Bar   \n",
      "10                          Berco's   \n",
      "11                              QBA   \n",
      "12                            Chido   \n",
      "13                      Cafe High 5   \n",
      "14                      Dasaprakash   \n",
      "15       Somewhere Restaurant & Bar   \n",
      "16                      38 Barracks   \n",
      "17         Out Of The Box Courtyard   \n",
      "18               The Imperial Spice   \n",
      "19  Ardor 2.1 Restaurant and Lounge   \n",
      "20                    The G.T. Road   \n",
      "\n",
      "                                              Cuisine  \\\n",
      "0                      North Indian,Asian,Continental   \n",
      "1              Continental,Asian,Italian,North Indian   \n",
      "2             Finger Food,Chinese,Continental,Italian   \n",
      "3             North Indian,Continental,American,Asian   \n",
      "4                          North Indian,Asian,Italian   \n",
      "5                        North Indian,Chinese,Italian   \n",
      "6                      Chinese,North Indian,Fast Food   \n",
      "7    North Indian,Italian,Chinese,Turkish,Continental   \n",
      "8          North Indian,Continental,Chinese,Fast Food   \n",
      "9              Italian,Chinese,North Indian,Fast Food   \n",
      "10                                       Chinese,Thai   \n",
      "11                   North Indian,Continental,Italian   \n",
      "12  North Indian,Italian,Continental,Asian,Finger ...   \n",
      "13                   North Indian,Continental,Chinese   \n",
      "14  North Indian,South Indian,Beverages,Chinese,Co...   \n",
      "15                     North Indian,Continental,Asian   \n",
      "16                   North Indian,Chinese,Continental   \n",
      "17         North Indian,Mediterranean,Chinese,Italian   \n",
      "18           North Indian,Chinese,Continental,Mughlai   \n",
      "19           North Indian,Chinese,Italian,Continental   \n",
      "20                                       North Indian   \n",
      "\n",
      "                                      Location Ratings  \\\n",
      "0   Scindia House,Connaught PlaceCentral Delhi       4   \n",
      "1                 Connaught PlaceCentral Delhi     4.2   \n",
      "2                 Connaught PlaceCentral Delhi     3.9   \n",
      "3         M-Block,Connaught PlaceCentral Delhi       4   \n",
      "4                 Connaught PlaceCentral Delhi     4.1   \n",
      "5                 Connaught PlaceCentral Delhi     4.1   \n",
      "6                 Connaught PlaceCentral Delhi     4.2   \n",
      "7                 Connaught PlaceCentral Delhi       4   \n",
      "8                 Connaught PlaceCentral Delhi     4.1   \n",
      "9         F-Block,Connaught PlaceCentral Delhi       4   \n",
      "10                Connaught PlaceCentral Delhi     4.3   \n",
      "11                Connaught PlaceCentral Delhi     4.2   \n",
      "12                Connaught PlaceCentral Delhi     4.2   \n",
      "13                Connaught PlaceCentral Delhi       4   \n",
      "14                Connaught PlaceCentral Delhi     4.2   \n",
      "15                Connaught PlaceCentral Delhi     4.1   \n",
      "16        M-Block,Connaught PlaceCentral Delhi     4.3   \n",
      "17                Connaught PlaceCentral Delhi     4.1   \n",
      "18        M-Block,Connaught PlaceCentral Delhi     4.4   \n",
      "19                Connaught PlaceCentral Delhi     4.1   \n",
      "20        M-Block,Connaught PlaceCentral Delhi     4.3   \n",
      "\n",
      "                                            Image URL  \n",
      "0   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "1   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "2   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "3   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "4   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "5   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "6   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "7   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "8   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "9   https://im1.dineout.co.in/images/uploads/resta...  \n",
      "10  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "11  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "12  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "13  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "14  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "15  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "16  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "17  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "18  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "19  https://im1.dineout.co.in/images/uploads/resta...  \n",
      "20  https://im1.dineout.co.in/images/uploads/resta...  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Send a GET request to the website\n",
    "url = \"https://www.dineout.co.in/delhi-restaurants\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Create a BeautifulSoup object to parse the HTML content\n",
    "soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "# Initialize lists to store the details\n",
    "restaurant_names = []\n",
    "cuisines = []\n",
    "locations = []\n",
    "ratings = []\n",
    "image_urls = []\n",
    "\n",
    "# Extract the details of each restaurant\n",
    "restaurants = soup.find(\"div\", class_=\"restnt-card-wrap-new\")\n",
    "\n",
    "for restaurant in restaurants:\n",
    "    # Extract restaurant name\n",
    "    name = restaurant.find(\"a\", class_=\"restnt-name ellipsis\").text.strip()\n",
    "    restaurant_names.append(name)\n",
    "    \n",
    "    # Extract cuisine\n",
    "    detailed_info = restaurant.find(\"div\", class_=\"detail-info\")\n",
    "    cuisines_list = detailed_info.find_all(\"a\")\n",
    "    cuisines.append(\",\".join(str(cusine.text.strip()) for cusine in cuisines_list))\n",
    "    \n",
    "    # Extract location\n",
    "    location_info = restaurant.find(\"div\", class_=\"restnt-loc ellipsis\")\n",
    "    location_list = location_info.find_all(\"a\")\n",
    "    locations.append(\"\".join(str(location.text.strip()) for location in location_list))\n",
    "    \n",
    "    # Extract image URL and rating\n",
    "    image_wraps = restaurant.find(\"div\",class_ =\"img-wrap\")\n",
    "    for idx,image_wrap in enumerate(image_wraps) :\n",
    "        if(idx == 0) :\n",
    "            image_url = image_wrap.find(\"img\", class_=\"lazy-load-img no-img\")['data-src']\n",
    "            image_urls.append(image_url)\n",
    "        elif(idx == 1) :\n",
    "            rating = image_wrap.text.strip()\n",
    "            ratings.append(rating)\n",
    "# Create a dataframe with the extracted details\n",
    "data = {\n",
    "    \"Restaurant Name\": restaurant_names,\n",
    "    \"Cuisine\": cuisines,\n",
    "    \"Location\": locations,\n",
    "    \"Ratings\": ratings,\n",
    "    \"Image URL\": image_urls\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Print the dataframe\n",
    "print(df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
